{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os.path\n",
    "path = \"/Users/lsesp/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import sklearn\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "df=pd.read_csv(\"IMDB Dataset/IMDB Dataset.csv\")\n",
    "\n",
    "sns.countplot(x=df[\"sentiment\"])\n",
    "\n",
    "df.head()\n",
    "#df.sentiment.value_counts().plot(kind=\"bar\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from transformers import InputExample, InputFeatures\n",
    "\n",
    "checkpoint_path= \"training/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path)\n",
    "\n",
    "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "model.save_pretrained(checkpoint_path, verbose = 1, save_weights_only = True)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing positive and negative into numeric values\n",
    "\n",
    "def cat2num(value):\n",
    "    if value=='positive': \n",
    "        return 1\n",
    "    else: \n",
    "        return 0\n",
    "    \n",
    "df['sentiment']  =  df['sentiment'].apply(cat2num)\n",
    "train = df[:45000]\n",
    "test = df[45000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['currently', 'enjoying', 'pin', '##oc', '##chio', 'pretty', 'good', 'film']\n",
      "[2747, 9107, 9231, 10085, 23584, 3492, 2204, 2143]\n"
     ]
    }
   ],
   "source": [
    "# But first see BERT tokenizer exmaples and other required stuff!\n",
    "\n",
    "example='Currently enjoying pinocchio pretty good film'\n",
    "tokens=tokenizer.tokenize(example)\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(tokens)\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data_to_examples(train, test, review, sentiment): \n",
    "    train_InputExamples = train.apply(lambda x: InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this case\n",
    "                                                          text_a = x[review], \n",
    "                                                          label = x[sentiment]), axis = 1)\n",
    "\n",
    "    validation_InputExamples = test.apply(lambda x: InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this case\n",
    "                                                          text_a = x[review], \n",
    "                                                          label = x[sentiment]), axis = 1,)\n",
    "  \n",
    "    return train_InputExamples, validation_InputExamples\n",
    "\n",
    "train_InputExamples, validation_InputExamples = convert_data_to_examples(train,  test, 'review',  'sentiment')\n",
    "                                                                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_InputExamples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_tf_dataset(examples, tokenizer, max_length=128):\n",
    "    features = [] # -> will hold InputFeatures to be converted later\n",
    "\n",
    "    for e in tqdm(examples):\n",
    "        input_dict = tokenizer.encode_plus(\n",
    "            e.text_a,\n",
    "            add_special_tokens=True,    # Add 'CLS' and 'SEP'\n",
    "            max_length=max_length,    # truncates if len(s) > max_length\n",
    "            return_token_type_ids=True,\n",
    "            return_attention_mask=True,\n",
    "            pad_to_max_length=True, # pads to the right by default # CHECK THIS for pad_to_max_length\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        input_ids, token_type_ids, attention_mask = (input_dict[\"input_ids\"],input_dict[\"token_type_ids\"], input_dict['attention_mask'])\n",
    "        features.append(InputFeatures( input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, label=e.label) )\n",
    "\n",
    "    def gen():\n",
    "        for f in features:\n",
    "            yield (\n",
    "                {\n",
    "                    \"input_ids\": f.input_ids,\n",
    "                    \"attention_mask\": f.attention_mask,\n",
    "                    \"token_type_ids\": f.token_type_ids,\n",
    "                },\n",
    "                f.label,\n",
    "            )\n",
    "\n",
    "    return tf.data.Dataset.from_generator(\n",
    "        gen,\n",
    "        ({\"input_ids\": tf.int32, \"attention_mask\": tf.int32, \"token_type_ids\": tf.int32}, tf.int64),\n",
    "        (\n",
    "            {\n",
    "                \"input_ids\": tf.TensorShape([None]),\n",
    "                \"attention_mask\": tf.TensorShape([None]),\n",
    "                \"token_type_ids\": tf.TensorShape([None]),\n",
    "            },\n",
    "            tf.TensorShape([]),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "DATA_COLUMN = 'review'\n",
    "LABEL_COLUMN = 'sentiment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/45000 [00:00<?, ?it/s]C:\\Users\\lsesp\\anaconda3\\envs\\proyectos\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2302: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████| 45000/45000 [04:42<00:00, 159.47it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data = convert_examples_to_tf_dataset(list(train_InputExamples), tokenizer)\n",
    "train_data = train_data.shuffle(100).batch(32).repeat(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:31<00:00, 158.02it/s]\n"
     ]
    }
   ],
   "source": [
    "validation_data = convert_examples_to_tf_dataset(list(validation_InputExamples), tokenizer)\n",
    "validation_data = validation_data.batch(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">Nuestro conjunto de datos que contiene secuencias de entrada procesadas está listo para alimentar el modelo.<font size=\"4\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "   2814/Unknown - 2265s 795ms/step - loss: 0.2418 - accuracy: 0.8988"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn while saving (showing 5 of 420). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training\\cp.ckpt\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training\\cp.ckpt\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2814/2814 [==============================] - 2351s 826ms/step - loss: 0.2418 - accuracy: 0.8988 - val_loss: 0.2938 - val_accuracy: 0.8964\n",
      "Epoch 2/2\n",
      "2814/2814 [==============================] - ETA: 0s - loss: 0.0719 - accuracy: 0.9750"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn while saving (showing 5 of 420). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training\\cp.ckpt\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training\\cp.ckpt\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n",
      "2814/2814 [==============================] - 2637s 937ms/step - loss: 0.0719 - accuracy: 0.9750 - val_loss: 0.4321 - val_accuracy: 0.8846\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f33ca9f0d0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0), \n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "              metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')])\n",
    "\n",
    "model.fit(train_data, epochs=2, validation_data=validation_data, callbacks = [cp_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>PRUEBA DEL MODELO ENTRENADO<font size=4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sentences = [\n",
    "                   \"this is one of the best things i’ve ever seen in my life\",\n",
    "                    \"Here’s the thing, though: Cats still makes no f.cking sense.\"\n",
    "                 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worst movie of my life, will never watch movies from this series :  Negative\n",
      "Wow, blew my mind, what a movie by Marvel, animation and story is amazing :  Positive\n"
     ]
    }
   ],
   "source": [
    "tf_batch = tokenizer(pred_sentences, max_length=128, padding=True, truncation=True, return_tensors='tf')   # we are tokenizing before sending into our trained model\n",
    "tf_outputs = model(tf_batch)                                  \n",
    "tf_predictions = tf.nn.softmax(tf_outputs[0], axis=-1)       # axis=-1, this means that the index that will be returned by argmax will be taken from the *last* axis.\n",
    "labels = ['Negative','Positive']\n",
    "label = tf.argmax(tf_predictions, axis=1)\n",
    "label = label.numpy()\n",
    "for i in range(len(pred_sentences)):\n",
    "    print(pred_sentences[i], \": \", labels[label[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>PRUEBA DEL MODELO ENTRENADO USANDO LOS PESOS GUARDADOS PREVIO (requiere lineas anteriores)<font size=4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest = tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x1f257908880>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "savedModel =  TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "savedModel.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sentences = [\n",
    "                   \"this is one of the best things i’ve ever seen in my life\",\n",
    "                    \"Here’s the thing, though: Cats still makes no f.cking sense.\"\n",
    "                 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is one of the best things i’ve ever seen in my piece of shit life :  Negative\n",
      "Here’s the thing, though: Cats still makes no f.cking sense. :  Negative\n"
     ]
    }
   ],
   "source": [
    "savedModel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0), \n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "              metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')])\n",
    "\n",
    "tf_batch2 = tokenizer(pred_sentences, max_length=128, padding=True, truncation=True, return_tensors='tf')   # we are tokenizing before sending into our trained model\n",
    "tf_outputs2 = model(tf_batch2)                                  \n",
    "tf_predictions2 = tf.nn.softmax(tf_outputs2[0], axis=-1)       # axis=-1, this means that the index that will be returned by argmax will be taken from the *last* axis.\n",
    "labels = ['Negative','Positive']\n",
    "label = tf.argmax(tf_predictions2, axis=1)\n",
    "label = label.numpy()\n",
    "for i in range(len(pred_sentences)):\n",
    "    print(pred_sentences[i], \": \", labels[label[i]])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
